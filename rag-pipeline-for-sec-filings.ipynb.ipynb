{
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Pyodide)",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "fbc7ea33-d43c-4a6e-a390-c416983c317f",
      "cell_type": "code",
      "source": "GOOGLE_API_KEY=\"AIzaSyADh3uaI2cy7snxXT1vAyVnoNoGuFUkUiU\"\n\nimport os\nimport re\nimport time\nimport json\nfrom datetime import datetime\nfrom typing import List, Dict, Any, Optional\nimport requests\nfrom bs4 import BeautifulSoup\nimport google.generativeai as genai\n# Hard cap so we don’t ship entire 10-Q into a single call\nMAX_SECTION_CHARS_FOR_LLM = 120_000\nLLM_REQ_SLEEP = 0.2\nGEMINI_MODEL = \"gemini-2.5-flash\"\n\nANNOTATION_SYSTEM = \"\"\"\\\nYou are a precise financial document tagger for SEC filings (10-K, 10-Q, 8-K, Forms 3/4/5).\nYour job is to prepare a consice summary not exceeding 1800 characters. Factual summary of THIS part, using only information present in the text (no speculation)\n\nRULES:\n- Do not fabricate numbers, periods, or items. Only extract what is present.\n- If a number lacks units/periods, leave those fields empty; do NOT guess.\n\"\"\"\n\ndef _build_annotation_user_prompt(section_text: str,\n                                  section_title: str,\n                                  form: str,\n                                  filed_at: str,\n                                  ticker: str) -> str:\n    header = f\"\"\"{ANNOTATION_SYSTEM}\n[CONTEXT]\nTicker: {ticker}\nForm: {form}\nFiled Date: {filed_at}\nSection Title: {section_title}\n\n[INSTRUCTIONS]\nOutput a single paragraph summary\n\n\"\"\"\n    return header + section_text\n\ndef annotate_and_split_with_llm(section: Dict[str, Any],\n                                ticker: str,\n                                filed_at: str,\n                                form: str,\n                                model_name: str = GEMINI_MODEL,\n                                timeout: int = 90) -> List[Dict[str, Any]]:\n    \"\"\"\n    Calls Gemini to split a section into annotated parts.\n    Returns a list of dicts: each with 'meta_header' and 'text' (verbatim).\n    Falls back to returning a single, unannotated part on failure.\n    \"\"\"\n    text = section.get(\"text\", \"\")[:MAX_SECTION_CHARS_FOR_LLM]\n    if not text.strip():\n        return []\n    model = genai.GenerativeModel(model_name)\n    genai.configure(api_key=GOOGLE_API_KEY)\n    prompt = _build_annotation_user_prompt(\n        section_text=text,\n        section_title=section.get(\"section_title\") or \"\",\n        form=form,\n        filed_at=filed_at,\n        ticker=ticker,\n    )\n    try:\n        resp = model.generate_content(\n            [\n                {\"role\": \"user\", \"parts\": [prompt]}\n            ],\n            safety_settings=None,\n            generation_config={\n                \"temperature\": 0.2, ## less temperature for factual summary\n                \"top_p\": 0.9,\n                \"max_output_tokens\": 4096 ## approx 3000 words\n            }\n        )\n        raw = resp.text\n    except Exception as e:\n        print(f\"[ERROR] Gemini call failed: {e}\")\n        \n    print(raw)\n    out: List[Dict[str, Any]] = []\n    summary = raw\n    \n    # Compact, human-readable header that still encodes rich tags\n    # Keep it short so it doesn’t eat too many tokens during embedding.\n    meta_bits = [\n        f\"TICKER={ticker}\",\n        f\"FORM={form}\",\n        f\"DATE={filed_at}\",\n    ]\n    if section.get(\"section_id\"):\n        meta_bits.append(f\"SEC={section['section_id']}\")\n\n    meta_header = \"[\" + \" | \".join(meta_bits) + \"]\"\n\n    if not summary:\n        print(f\"[INFO] No summary returned by gemini for: {meta_header}\")\n    else:\n        print(\"summary: \", summary)\n        out.append({\"meta_header\": meta_header, \"text\": text, \"summary\": summary})\n\n    return out\n\n\n# ==== CELL A: INDEX (multiple tickers & multiple forms in a date range) ====\nimport os\nimport re\nimport time\nfrom datetime import datetime\nfrom typing import List, Dict, Any, Optional\nimport requests\nfrom bs4 import BeautifulSoup\nimport json  # <-- for saving artifacts\n\n# --- sec-parser ---\nimport warnings\nimport sec_parser as sp\nfrom sec_parser.semantic_tree import TreeBuilder\nfrom sec_parser.semantic_elements import TopSectionTitle\nfrom sec_parser.processing_steps import (\n    TopSectionManagerFor10Q,\n    IndividualSemanticElementExtractor,\n    TopSectionTitleCheck\n)\n\n# --- vector store & embeddings ---\nimport chromadb\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import AutoTokenizer\n\n# =========================\n# CONFIG\n# =========================\nUSER_AGENT = \"Your Name your_email@example.com\"  # <-- use a real UA\nCHROMA_DIR = \"./chroma_sec_multi\"  #  store path\nEMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\nGEMINI_MODEL = \"gemini-2.5-flash\"\nMAX_TOKENS = 512\nREQ_SLEEP = 0.25  # be kind to SEC endpoints (<= 10 req/sec)\nTICKERS = [\"AAPL\"]  # <-- multi-ticker support\nFORMS = [\"10-Q\",\"10-K\",\"8-K\",\"3\",\"4\",\"5\"]  # <-- choose which forms \"10-Q\", \"10-K\", \"8-K\",\"4\", \"5\"\nSTART_DATE = \"2025-06-01\"\nEND_DATE = \"2025-08-01\"\n\n# =========================\n# SEC helpers (no API key)\n# =========================\ndef _headers():\n    return {\n        \"User-Agent\": USER_AGENT,\n        \"Accept-Encoding\": \"gzip, deflate\",\n        \"Accept-Language\": \"en-US,en;q=0.9\",\n        \"Connection\": \"keep-alive\",\n    }\n\n## cik is an unique identifier assigned by the SEC to each company, ideally we should use cik to fetch filings.\ndef get_cik_for_ticker(ticker: str) -> Dict[str, str]:\n    url = \"https://www.sec.gov/files/company_tickers.json\"\n    r = requests.get(url, headers=_headers(), timeout=60)\n    r.raise_for_status()\n    data = r.json()\n    t_up = ticker.upper()\n    \n    for _, row in data.items():\n        if row.get(\"ticker\", \"\").upper() == t_up:\n            cik = str(int(row[\"cik_str\"]))\n            return {\"cik\": cik, \"cik_padded\": f\"{int(cik):010d}\"}\n    \n    raise ValueError(f\"Ticker {ticker} not found in SEC company_tickers.json\")\n\ndef list_filings_in_range(ticker: str, forms: List[str], start_date: str, end_date: str) -> List[Dict[str, str]]:\n    info = get_cik_for_ticker(ticker)\n    cik, cik_padded = info[\"cik\"], info[\"cik_padded\"]\n    \n    def _form_matches(f: str, forms: List[str]) -> bool:\n        for frm in forms:\n            if frm in (\"10-K\", \"10-Q\", \"8-K\"):\n                if f.startswith(frm):  # allow \"10-K/A\", etc.\n                    return True\n            else:\n                if f == frm:  # exact match for 3, 4, 5\n                    return True\n        return False\n    \n    def _collect(sub_json: dict, acc: List[Dict[str, str]]):\n        recent = sub_json.get(\"filings\", {}).get(\"recent\", {})\n        forms_list = recent.get(\"form\", [])\n        dates_list = recent.get(\"filingDate\", [])\n        accnos = recent.get(\"accessionNumber\", [])\n        primary = recent.get(\"primaryDocument\", [])\n        \n        for f, d, a, p in zip(forms_list, dates_list, accnos, primary):\n            if not f or not d or not a or not p:\n                continue\n            # match any of our requested forms (prefix match for variants like \"10-K/A\", \"8-K/A\")\n            if not _form_matches(f, forms):\n                continue\n            accnod = a.replace(\"-\", \"\")\n            url = f\"https://www.sec.gov/Archives/edgar/data/{cik}/{accnod}/{p}\"\n            acc.append({\"url\": url, \"filed_at\": d, \"accession\": a, \"form\": f, \"primary\": p})\n    \n    # main submissions\n    base = f\"https://data.sec.gov/submissions/CIK{cik_padded}.json\"\n    r = requests.get(base, headers=_headers(), timeout=60)\n    r.raise_for_status()\n    sub = r.json()\n    time.sleep(REQ_SLEEP)\n    \n    rows: List[Dict[str, str]] = []\n    _collect(sub, rows)\n    \n    # older files index pages\n    for f in sub.get(\"filings\", {}).get(\"files\", []):\n        name = f.get(\"name\")\n        if not name:\n            continue\n        url = f\"https://data.sec.gov/submissions/{name}\"\n        rr = requests.get(url, headers=_headers(), timeout=60)\n        if rr.status_code == 200:\n            _collect(rr.json(), rows)\n        time.sleep(REQ_SLEEP)\n    \n    # date filter inclusive\n    sd = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n    ed = datetime.strptime(end_date, \"%Y-%m-%d\").date()\n    rows = [\n        r for r in rows\n        if sd <= datetime.strptime(r[\"filed_at\"], \"%Y-%m-%d\").date() <= ed\n    ]\n    \n    # de-dup by accession; sort oldest→newest\n    seen = set()\n    uniq = []\n    for r in sorted(rows, key=lambda x: x[\"filed_at\"]):\n        if r[\"accession\"] in seen:\n            continue\n        seen.add(r[\"accession\"])\n        uniq.append(r)\n    \n    return uniq\n\ndef normalize_edgar_url(url: str) -> str:\n    # handle inline XBRL viewer links: /ix?doc=/Archives/...\n    from urllib.parse import urlparse, parse_qs\n    try:\n        p = urlparse(url)\n        if p.path.lower() == \"/ix\":\n            doc = parse_qs(p.query).get(\"doc\", [None])[0]\n            if doc:\n                return \"https://www.sec.gov\" + doc if doc.startswith(\"/\") else doc\n    except Exception:\n        pass\n    return url\n\ndef fetch_html(url: str) -> str:\n    url = normalize_edgar_url(url)\n    r = requests.get(url, headers=_headers(), timeout=90, allow_redirects=True)\n    r.raise_for_status()\n    time.sleep(REQ_SLEEP)\n    return r.text\n\n# =========================\n# Parsing by form\n# =========================\nITEM_RE = re.compile(r\"\\bItem\\s+\\d+[A]?(?:\\.\\d+)?\\b\", re.IGNORECASE)  # supports 8-K item 2.02 etc.\n\ndef _parse_items_10q(html: str) -> List[Dict[str, Any]]:\n    \"\"\"Full Item bodies for 10-Q using standard parser.\"\"\"\n    parser = sp.Edgar10QParser()\n    elements = parser.parse(html)\n    tree = TreeBuilder().build(elements)\n    nodes = list(tree.nodes)\n    \n    # find Item titles\n    item_idx = []\n    for i, node in enumerate(nodes):\n        if isinstance(node.semantic_element, TopSectionTitle):\n            title = (node.text or \"\").strip()\n            if ITEM_RE.search(title):\n                item_idx.append((i, title))\n    \n    if not item_idx:\n        for i, node in enumerate(nodes):\n            title = (node.text or \"\").strip()\n            if ITEM_RE.match(title):\n                item_idx.append((i, title))\n    \n    sections = []\n    for j, (start_i, title) in enumerate(item_idx):\n        end_i = item_idx[j + 1][0] if j + 1 < len(item_idx) else len(nodes)\n        html_parts, text_parts = [], []\n        \n        for k in range(start_i, end_i):\n            n = nodes[k]\n            try:\n                html_parts.append(n.get_source_code(pretty=False))\n            except Exception:\n                pass\n            t = (n.text or \"\").strip()\n            if t:\n                text_parts.append(t)\n        \n        html_chunk = \"\".join(html_parts).strip()\n        text_chunk = (\n            BeautifulSoup(html_chunk, \"html.parser\").get_text(separator=\"\\n\").strip()\n            if html_chunk\n            else \"\\n\".join(text_parts).strip()\n        )\n        \n        m = re.search(r\"\\bItem\\s+(\\d+[A]?(?:\\.\\d+)?)\\b\", title, flags=re.IGNORECASE)\n        section_id = m.group(1).upper() if m else None\n        title_norm = re.sub(r\"\\s+\", \" \", title.replace(\"\\xa0\", \" \")).strip()\n        text_chunk = re.sub(r\"\\n{3,}\", \"\\n\\n\", text_chunk.replace(\"\\xa0\", \" \")).strip()\n        \n        if text_chunk:\n            sections.append({\n                \"section_id\": section_id,\n                \"section_title\": title_norm,\n                \"html\": html_chunk or title_norm,\n                \"text\": text_chunk\n            })\n    \n    return sections\n\ndef _get_steps_10k_generic():\n    \"\"\"\n    Remove 10-Q specific top-section logic as per sec-parser docs.\n    \"\"\"\n    all_steps = sp.Edgar10QParser().get_default_steps()\n    steps_wo_top_mgr = [st for st in all_steps if not isinstance(st, TopSectionManagerFor10Q)]\n    \n    def get_checks_without_top_section_title_check():\n        checks = sp.Edgar10QParser().get_default_single_element_checks()\n        return [ck for ck in checks if not isinstance(ck, TopSectionTitleCheck)]\n    \n    return [\n        IndividualSemanticElementExtractor(get_checks=get_checks_without_top_section_title_check)\n        if isinstance(st, IndividualSemanticElementExtractor)\n        else st\n        for st in steps_wo_top_mgr\n    ]\n\ndef _parse_items_10k(html: str) -> List[Dict[str, Any]]:\n    \"\"\"Parse 10-K by skipping 10-Q-specific steps, then split by Item titles.\"\"\"\n    parser = sp.Edgar10QParser(get_steps=_get_steps_10k_generic)\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", message=\"Invalid section type for\")\n        elements = parser.parse(html)\n    \n    tree = TreeBuilder().build(elements)\n    nodes = list(tree.nodes)\n    \n    # find Item titles (10-K has Item 1..15)\n    item_idx = []\n    for i, node in enumerate(nodes):\n        title = (node.text or \"\").strip()\n        if ITEM_RE.match(title):\n            item_idx.append((i, title))\n    \n    sections = []\n    for j, (start_i, title) in enumerate(item_idx):\n        end_i = item_idx[j + 1][0] if j + 1 < len(item_idx) else len(nodes)\n        html_parts, text_parts = [], []\n        \n        for k in range(start_i, end_i):\n            n = nodes[k]\n            try:\n                html_parts.append(n.get_source_code(pretty=False))\n            except Exception:\n                pass\n            t = (n.text or \"\").strip()\n            if t:\n                text_parts.append(t)\n        \n        html_chunk = \"\".join(html_parts).strip()\n        text_chunk = (\n            BeautifulSoup(html_chunk, \"html.parser\").get_text(separator=\"\\n\").strip()\n            if html_chunk\n            else \"\\n\".join(text_parts).strip()\n        )\n        \n        m = re.search(r\"\\bItem\\s+(\\d+[A]?)\\b\", title, flags=re.IGNORECASE)\n        section_id = m.group(1).upper() if m else None\n        title_norm = re.sub(r\"\\s+\", \" \", title.replace(\"\\xa0\", \" \")).strip()\n        text_chunk = re.sub(r\"\\n{3,}\", \"\\n\\n\", text_chunk.replace(\"\\xa0\", \" \")).strip()\n        \n        if text_chunk:\n            sections.append({\n                \"section_id\": section_id,\n                \"section_title\": title_norm,\n                \"html\": html_chunk or title_norm,\n                \"text\": text_chunk\n            })\n    \n    return sections\n\ndef _parse_items_8k(html: str) -> List[Dict[str, Any]]:\n    \"\"\"\n    8-K Items are like 'Item 2.02 Results of Operations and Financial Condition'.\n    We'll split on these titles similarly.\n    \"\"\"\n    # Reuse 10-K generic parser (no 10-Q top manager)\n    parser = sp.Edgar10QParser(get_steps=_get_steps_10k_generic)\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", message=\"Invalid section type for\")\n        elements = parser.parse(html)\n    \n    tree = TreeBuilder().build(elements)\n    nodes = list(tree.nodes)\n    \n    item_idx = []\n    for i, node in enumerate(nodes):\n        title = (node.text or \"\").strip()\n        if re.match(r\"\\bItem\\s+\\d+\\.\\d+\\b\", title, flags=re.IGNORECASE):\n            item_idx.append((i, title))\n    \n    # fallback: any \"Item x.xx\" found anywhere\n    if not item_idx:\n        for i, node in enumerate(nodes):\n            title = (node.text or \"\").strip()\n            if re.search(r\"\\bItem\\s+\\d+\\.\\d+\\b\", title, flags=re.IGNORECASE):\n                item_idx.append((i, title))\n    \n    sections = []\n    for j, (start_i, title) in enumerate(item_idx):\n        end_i = item_idx[j + 1][0] if j + 1 < len(item_idx) else len(nodes)\n        html_parts, text_parts = [], []\n        \n        for k in range(start_i, end_i):\n            n = nodes[k]\n            try:\n                html_parts.append(n.get_source_code(pretty=False))\n            except Exception:\n                pass\n            t = (n.text or \"\").strip()\n            if t:\n                text_parts.append(t)\n        \n        html_chunk = \"\".join(html_parts).strip()\n        text_chunk = (\n            BeautifulSoup(html_chunk, \"html.parser\").get_text(separator=\"\\n\").strip()\n            if html_chunk\n            else \"\\n\".join(text_parts).strip()\n        )\n        \n        m = re.search(r\"\\bItem\\s+(\\d+\\.\\d+)\\b\", title, flags=re.IGNORECASE)\n        section_id = m.group(1) if m else None\n        title_norm = re.sub(r\"\\s+\", \" \", title.replace(\"\\xa0\", \" \")).strip()\n        text_chunk = re.sub(r\"\\n{3,}\", \"\\n\\n\", text_chunk.replace(\"\\xa0\", \" \")).strip()\n        \n        if text_chunk:\n            sections.append({\n                \"section_id\": section_id,\n                \"section_title\": title_norm,\n                \"html\": html_chunk or title_norm,\n                \"text\": text_chunk\n            })\n    \n    return sections\n\ndef _parse_whole_doc(html: str) -> List[Dict[str, Any]]:\n    \"\"\"For Forms 3/4/5 (insider forms), treat the entire doc as a single section.\"\"\"\n    soup = BeautifulSoup(html, \"html.parser\")\n    text = soup.get_text(separator=\"\\n\").strip()\n    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text.replace(\"\\xa0\", \" \"))\n    \n    if not text:\n        return []\n    \n    return [{\n        \"section_id\": \"ALL\",\n        \"section_title\": \"Entire Document\",\n        \"html\": html,\n        \"text\": text\n    }]\n\ndef parse_sections_by_form(html: str, form: str) -> List[Dict[str, Any]]:\n    base = form.upper()\n    if base.startswith(\"10-Q\"):\n        return _parse_items_10q(html)\n    if base.startswith(\"10-K\"):\n        return _parse_items_10k(html)\n    if base.startswith(\"8-K\"):\n        return _parse_items_8k(html)\n    if base in {\"3\", \"4\", \"5\"}:\n        return _parse_whole_doc(html)\n    # default fallback\n    return _parse_whole_doc(html)\n\n# =========================\n# Chunk, embed, upsert (metadata inside embedding text)\n# =========================\n# def chunk_text(s: str, max_chars: int = 1800, overlap: int = 200) -> List[str]:\n#     s = s.strip()\n#     if len(s) <= max_chars:\n#         return [s]\n    \n#     chunks, start = [], 0\n#     while start < len(s):\n#         end = min(len(s), start + max_chars)\n#         split = s.rfind(\"\\n\\n\", start, end)\n#         if split == -1 or split <= start + 200:\n#             split = end\n#         chunks.append(s[start:split].strip())\n#         if split == len(s):\n#             break\n#         start = max(split - overlap, split)\n    \n#     return [c for c in chunks if c]\n\n# def _trim_to_tokens(text: str, tokenizer, max_tokens: int) -> str:\n#     enc = tokenizer(text, add_special_tokens=False, return_attention_mask=False, return_token_type_ids=False)\n#     ids = enc[\"input_ids\"]\n#     if len(ids) <= max_tokens:\n#         return text\n#     trimmed = tokenizer.decode(ids[:max_tokens], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n#     return trimmed.strip()\n\ndef build_chroma(persist_dir: str = CHROMA_DIR):\n    client = chromadb.Client()\n    coll = client.get_or_create_collection(name=\"sec_multi_forms\")\n    return client, coll\n    \ndef upsert_sections_to_chroma(\n    coll,\n    sections: List[Dict[str, Any]],\n    ticker: str,\n    filed_at: str,\n    form: str,\n    model: SentenceTransformer\n) -> int:\n    ids, docs, metas, embs = [], [], [], []\n    uid = 0\n\n    for s in sections:\n        # ===== NEW: ask LLM to split+annotate this section =====\n        try:\n            annotated_parts = annotate_and_split_with_llm(\n                section=s,\n                ticker=ticker,\n                filed_at=filed_at,\n                form=form,\n                model_name=GEMINI_MODEL\n            )\n        except Exception:\n            annotated_parts = []\n\n        # Fallback to previous mono-section behavior if LLM unavailable\n        if not annotated_parts:\n            continue\n        else:\n            # base_meta_str = (\n            #     f\"[TICKER={ticker}|FORM={form}|DATE={filed_at}\"\n            #     f\"{'|SEC='+str(s.get('section_id')) if s.get('section_id') else ''}] \"\n            #     f\"{(s.get('section_title') or '')[:120]}\"\n            # ).strip()\n            # annotated_parts = [{\"meta_header\": base_meta_str, \"text\": s.get(\"text\",\"\")}]\n            for part in annotated_parts:\n                header = part[\"meta_header\"].strip()\n                part_text = part[\"text\"].strip()\n                part_summary = part[\"summary\"].strip() if \"summary\" in part else \"\"\n                if not part_summary:\n                    print(f\"[INFO] No summary found for: {header}\")\n            \n                meta = {\n                    \"ticker\": ticker,\n                    \"form\": form,\n                    \"filed_at\": filed_at,\n                    \"section_id\": s.get(\"section_id\"),\n                    \"section_title\": s.get(\"section_title\")\n                }\n            \n                # --- Save chunk metadata, summary, and text before embedding ---\n                artifact = {\n                    \"meta_header\": header,\n                    \"meta\": meta,\n                    \"summary\": part_summary,\n                    \"text\": part_text\n                }\n                artifact_path = os.path.join(\n                    ARTIFACT_DIR,\n                    f\"{ticker}_{form}_{filed_at}_{s.get('section_id')}_{uid:06d}_chunk.json\"\n                )\n                with open(artifact_path, \"w\", encoding=\"utf-8\") as af:\n                    json.dump(artifact, af, indent=2, ensure_ascii=False)\n            \n                doc_text = f\"{header}\\n\\n{meta}\\n\\n{part_summary}\".strip()\n            \n                emb = model.encode(doc_text, normalize_embeddings=True).tolist()\n                # ...existing code...\n                ids.append(f\"{ticker}_{form}_{filed_at}_{s.get('section_id')}_{uid:06d}\")\n                docs.append(doc_text)\n                metas.append(meta)\n                embs.append(emb)\n                uid += 1\n\n    if ids:\n        coll.upsert(ids=ids, documents=docs, metadatas=metas, embeddings=embs)\n    return len(ids)\n\n\n# =========================\n# RUN: iterate tickers & forms over date range\n# =========================\nclient, collection = build_chroma(CHROMA_DIR)\nmodel = SentenceTransformer(EMBED_MODEL_NAME)\ntokenizer = AutoTokenizer.from_pretrained(EMBED_MODEL_NAME)\n\n# Directory to store artifacts\nARTIFACT_DIR = \"form_artifacts\"\nos.makedirs(ARTIFACT_DIR, exist_ok=True)\n\ntotal = 0\nfor tkr in TICKERS:\n    filings = list_filings_in_range(tkr, FORMS, START_DATE, END_DATE)\n    print(f\"\\n{tkr}: {len(filings)} filings from {START_DATE} to {END_DATE}\")\n    \n    for f in filings:\n        print(\"  \", f[\"filed_at\"], f[\"form\"], \"→\", f[\"url\"])\n        html = fetch_html(f[\"url\"])\n\n        # --- Save raw HTML (pre-parsing) ---\n        raw_path = os.path.join(ARTIFACT_DIR, f\"{tkr}_{f['form']}_{f['filed_at']}_raw.html\")\n        with open(raw_path, \"w\", encoding=\"utf-8\") as fh:\n            fh.write(html)\n\n        sections = parse_sections_by_form(html, f[\"form\"])\n\n        # --- Save parsed sections (post-parsing) ---\n        parsed_path = os.path.join(ARTIFACT_DIR, f\"{tkr}_{f['form']}_{f['filed_at']}_sections.json\")\n        with open(parsed_path, \"w\", encoding=\"utf-8\") as fp:\n            json.dump(sections, fp, indent=2, ensure_ascii=False)\n\n        # --- Save final chunks (post-embedding) ---\n        upsert_sections_to_chroma(collection, sections, tkr, f[\"filed_at\"], f[\"form\"], model)\nprint(f\"\\nDONE. Total chunks upserted: {total}. DB: {CHROMA_DIR}\")\n\n\n\nimport os\nimport json\nfrom typing import List, Dict, Any, Optional\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import AutoTokenizer\nimport chromadb\nimport google.generativeai as genai\n\n# Reuse the same settings used at index time\nCHROMA_DIR = \"./chroma_sec_multi\"\nEMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\nGEMINI_MODEL = \"gemini-2.5-flash\"   # or \"gemini-1.5-pro\"\nTOP_K = 5\n\ndef load_chroma():\n    client = chromadb.Client()\n    return client.get_or_create_collection(name=\"sec_multi_forms\")\n\ndef retrieve_top_k(coll, query: str, k: int = TOP_K):\n    model = SentenceTransformer(EMBED_MODEL_NAME)\n    q_emb = model.encode(query, normalize_embeddings=True).tolist()\n    res = coll.query(query_embeddings=[q_emb], n_results=k)\n    hits = []\n    if res and res.get(\"documents\"):\n        for i in range(len(res[\"documents\"][0])):\n            hits.append({\n                \"id\": res[\"ids\"][0][i],\n                \"text\": res[\"documents\"][0][i],\n                \"meta\": res[\"metadatas\"][0][i],\n                \"distance\": res[\"distances\"][0][i],  # cosine distance (smaller is better)\n            })\n    hits.sort(key=lambda x: x[\"distance\"])\n    return hits\n\ndef gemini_answer(query: str, contexts: List[Dict[str, Any]]) -> str:\n    api_key = GOOGLE_API_KEY\n    if not api_key:\n        raise RuntimeError(\"Set GOOGLE_API_KEY env var for Gemini.\")\n    genai.configure(api_key=api_key)\n    model = genai.GenerativeModel(GEMINI_MODEL)\n\n    blocks = []\n    for c in contexts:\n        m = c.get(\"meta\", {})\n        tag = f\"[{m.get('ticker')} | {m.get('form')} | {m.get('filed_at')} | {m.get('section_title')} | chunk {m.get('chunk_index')}]\"\n        blocks.append(f\"{tag}\\n{c['text']}\\n\")\n\n    prompt = (\n        \"You are a financial research assistant.\\n\"\n        \"Use the provided SEC excerpts to answer the user question. Use no data except the one provided in the context.\"\n        \"Cite the section titles inline, and be concise. If uncertain, say so.\\n\\n\"\n        \"whenever numerical values need to be analysed or compared give output in markdown tabular format\"\n        f\"USER QUESTION:\\n{query}\\n\\n\" +\n        \"\\n\".join(f\"CONTEXT {i+1}:\\n{blk}\" for i, blk in enumerate(blocks[:TOP_K]))\n    )\n\n    resp = model.generate_content(prompt)\n    return resp.text\n\n# ======= RUN THIS CELL MULTIPLE TIMES =======\ncollection = load_chroma()\n\nuser_query = \"khan sabih initial rsu holding data\"\nhits = retrieve_top_k(collection, user_query, k=10)\nprint(\"Top hits:\\n\", json.dumps(hits, indent=2))\n\nanswer = gemini_answer(user_query, hits[:2])\nprint(\"\\n=== GEMINI ANSWER ===\\n\")\nprint(answer)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}